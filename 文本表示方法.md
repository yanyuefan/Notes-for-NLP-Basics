- 3、8、10

## 文本表示方法

#### 语言模型

- 用概率 p(s) 来表示字符串 s 作为1个句子出现的频率
- 词汇表单词数V，句子平均长度为M ，时间复杂度为$O(V^M)$

#### 基于one-hot、tf-idf、textrank

- TF-IDF是一种统计方法，用以评估**一字词**对于**一个文件集**或一个语料库中的**其中一份文件**的重要程度。
  - **字词的重要性**随着它在**文件中出现的次数成正比**增加，但同时会随着它在**语料库中出现的频率成反比**下降。
  - 词频  TF（Term Frequency）
    - 某一个给定的词语在该文件中出现的次数
    - 这个数字通常会被**归一化**
      - 一般是词频除以文章总词数
      - 防止它偏向长的文件。
  - 逆文档频率 IDF（Inverse Document Frequency）
    - 文档频率【word出现的文档数/文件集文档数】的倒数
    - log（语料库的文档总数/包含该词的文档数）
    - 如果包含词条 t 的文档越少，也就是n越小，IDF越大，则说明词条t具有很好的**类别区分能力**
  - 特点：
    - 词袋模型，所以没有考虑词的位置信息
    - 使罕见的单词更加突出并且有效地忽略了常用单词 

#### 主题模型

- LDA（Latent Dirichlet Allocation）隐含狄利克雷分布
  - 文档集中每篇文档的主题：概率分布
- LSA（Latent Semantic Analysis）潜在语义分析
  - **文档与所有词**在该文档中出现频次的矩阵（词文档矩阵）
  - 矩阵分解
    - SVD分解，左边的矩阵就是词向量
- pLSA（Probability Latent Semantic Analysis）概率潜在语义分析

#### NNLM 神经网络语言模型

- 利用上下文词预测中心词，利用前n个词预测下一个词
- 初始化一个V*D的词向量矩阵，其中V是语料库词汇表的大小，D是词向量的维度，随机初始值
- **输入层：**利用n个上下文词的one-hot编码与词向量矩阵相乘，找到每个单词的词向量，拼接起来，成为一个更长的向量，即D*n维向量
- **隐藏层：**将D*n维向量经过线性层转换为M维向量，再经过tanh等激活函数
- **输出层：**再把M维向量经过线性层转换为大小为V维的向量，经过softmax函数即可得到每一个词的概率

![img](https://pic3.zhimg.com/80/v2-e493a7f8f33049e6660105215869ddde_720w.jpg)

- 其**训练目标**是最大化似然函数
  - 似然函数
    - 关于参数的函数
    - 给定输出x时，关于参数θ的似然函数L(θ|x)（在数值上）= **给定参数θ**后，变量X的概率：L(θ|x) = P(X=x|θ)。
    - **概率**描述了已知参数时的随机变量的输出结果；**似然**则用来描述**已知**随机变量**输出结果**时，未知**参数的可能取值**。
    - 似然函数取得最大值表示相应的参数能够使得统计模型最为合理
  - **最大似然函数：**在参数θ的所有取值上，使这个函数最大化。这个使可能性最大的值即被称为θ的最大似然估计。

#### Word2vec 和nnlm对比

- 词向量只是nnlm的一个产物，word2vec更专注于词向量本身

- 具体计算方面，在利用上下文词预测中心词时，nnlm是把上下文词的向量进行拼接，word2vec是进行sum，并舍弃隐藏层（为了减少计算量）

#### word2vec

- hierarchical softmax 层次softmax（利用CBOW）：
  - **输入层**：利用n个上下文词的one-hot编码与词向量矩阵相乘，找到每个单词的词向量
  - **投影层：**将n个上下文词的词向量相加求和得到X
  - **输出层：**对应一个哈夫曼树，以每个词在语料库中的频次构造出来的哈夫曼树，**叶节点有V个，即词汇表的大小**。非叶子节点有V-1个，图中黄色部分。
  - **哈夫曼树详解：**每一个叶节点有对应的唯一从根节点出发路径，可用0，1进行编码，左1右0。**路径上的非叶子节点都对应一个向量，该向量是参数向量，最终更新的也是该向量**。
  - **如何确定条件概率：**现有一个上下文向量X，经过未知节点之后找到正确的中心词，在找的过程中，每经过一个子节点，都要进行一次判断，看是往左走还是往右走，刚好哈夫曼树左为1右为0，即每走一步都可看作是一个二分类，总结一句话就是：**上下文词找到中心词的过程就是上下文词经过多个二分类之后最终到达目标词，每一步都要进行判断。**
  - **时间复杂度：**从O（N）降到O（log（N））
  - https://zhuanlan.zhihu.com/p/75391062
- hierarchical softmax 层次softmax（利用Skip-gram）
  - **输入层**：只含当前词的词向量x
  - **投影层：**没什么用，只是为了和CBOW对比
  - **输出层：**对应一个哈夫曼树，以每个词在语料库中的频次构造出来的哈夫曼树，叶节点有V个，即词汇表的大小。非叶子节点有V-1个，图中黄色部分。
  - **与CBOW的区别：**目标函数有三个连加，**最内层**为每一个被预测词从根节点走下来的概率，**外面一层**是一个中心词预测的多个上下文词的概率，**最外面**是确保整个训练集的预测正确的概率![img](https://pic1.zhimg.com/80/v2-c603c649ed387bbd58a33f5bf0ea7003_720w.jpg)
  - **更新时：**每一个Theta j都对应一个w的偏导，所以要求和，另外每一个上下文词u也对应一个w的偏导，再次求和。![img](https://picb.zhimg.com/80/v2-e4c5a3330aacbf2529f326c2929f7c46_720w.jpg)

-  深入剖析nagative sampling（利用CBOW）
-  深入剖析nagative sampling（利用Skip-gram）

#### Glove（Global Vectors for Word Representation）

- 一个基于全局词频统计（count-based & overall statistics）的词表征（word representation）工具
- 目标：进行词的向量化表示，使得向量之间尽可能多地蕴含语义和语法的信息
- 输入：语料库
- 输出：词向量
- 方法：首先根据语料库**构建共现矩阵**，然后基于共现矩阵和Glove模型学习词向量
- 构建共现矩阵
  - 首先构建一个空矩阵，大小为VxV，即词汇表*词汇表，值全为空
  - 确定一个滑动窗口的大小，比如：5
  - 从语料库的第一个单词开始，滑动该窗口
  - 因为是按照语料库的顺序开始的，所以中心词为到达的那个单词：i
  - 上下文词为以中心词i为滑动窗口的中心，两边的单词，为上下文词：j_1, j_2,…,j_m（假如有m个j）
  - 若窗口左右无单词，一般出现在语料库的首尾，则空着，不需要统计
  - 在窗口内，统计上下文词j出现的次数，添在矩阵中
  - 滑动过程中，不断将统计的信息添加在矩阵中，即可得到共现矩阵
- **共现概率矩阵**
  - 每两个单词都可以得到一个这样的矩阵
    - 其中第一行以某个单词i为中心词，其他V个词，记为k，出现的概率，用k的count / i的count 表示概率
    - 第二行与第一行类似，只是此时统计的是中心词j。
    - 第三行为第一行 / 第二行![img](https://pic2.zhimg.com/80/v2-fb24e4761cd3f1658d91f61e53f0be88_720w.jpg)
    - 共现矩阵为 $X$, $X_{ij}$表示在词i的上下文中j出现的次数， $P_{ij}=P(j|i)=X_{ij}/X_{i}$
      - 对词i,j,k，若**k与i相关**而与j无关，$P_{ik}/P_{jk}$应较大；若**k与j相关**而与i无关，那么$P_{ik}/P_{jk}$应较小；若**k与i,j均无关**，则$P_{ik}/P_{jk}$接近1.
  - 比率（Ratio）的作用：三个词的关系![img](https://pic2.zhimg.com/80/v2-026b2170e9eb6efef00684653f4ab2e4_720w.jpg)
- 思路
  - 若三个词向量经过某种计算（ratio），可以和共现矩阵具有一致性；即词向量蕴含了共现矩阵中蕴含的信息
- 损失函数
  - ![img](https://pic2.zhimg.com/80/v2-5545d6a62da6276e7b36ae3c5d90ae20_720w.jpg)
  - 其中f(x)为权重函数，即频率越高的词权重应该越大，这篇论文中的所有实验，α的取值都是0.75，而xmax取值都是100
- 根据两个单词在上下文窗口的距离\(d\)，提出了一个衰减函数（decreasing weighting）：\(decay = 1/d\)用于计算权重
  
- 训练方式
  - label：$log(X_{i,j})$ [共现次数]
  - 学习更新的参数：$v_i,v_j$
    - 该参数即为需要的词向量
  - 基于梯度下降
  - 论文
    - 采用了`AdaGrad`的梯度下降算法
    - 对矩阵 X 中的所有非零元素进行随机采样
    - 学习曲率（learning rate）设为0.05
    - 在vector size小于300的情况下迭代了50次
    - 其他大小的vectors上迭代了100次，直至收敛
    - 结果
      - 最终学习得到$v_i,v_j$，因为 X 是对称的（symmetric），所以从原理上讲 $v_i $和 $v_j $是也是对称的，他们唯一的区别是初始化的值不一样，而导致最终的值不一样
      - 最终会选择两者之和 $v_i + v_j $作为最终的vector（两者的初始化不同相当于加了不同的随机噪声，所以能提高鲁棒性）

#### **Glove与LSA比较**

- 两者都是基于共现矩阵在操作
- LSA（Latent Semantic Analysis）可以基于co-occurance matrix构建词向量，实质上是基于全局语料采用SVD进行矩阵分解，然而SVD计算复杂度高
- Glove没有直接利用共现矩阵，而是通过ratio的特性，将词向量和ratio联系起来，建立损失函数，采用Adagrad对最小平方损失进行优化（可看作是对LSA一种优化的高效矩阵分解算法）

#### **Glove与Word2vec比较**

- Word2vec是局部语料库训练的，其特征提取是基于滑窗的；
- glove的滑窗是为了构建co-occurance matrix（上面详细描述了窗口滑动的过程），统计了全部语料库里在固定窗口内的词共线的频次，是基于全局语料的，可见glove需要事先统计共现概率；
- Word2vec**损失函数**实质上是带权重的交叉熵，权重固定；glove的**损失函数**是最小平方损失函数，权重可以做映射变换。
- Glove利用了全局信息，使其在训练时收敛更快，训练周期较word2vec较短且效果更好